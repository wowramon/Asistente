{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Asistente Para HelpDesk",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00001-a29ddb4a-990a-4cc2-aad8-6a2a19234f63",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "tags": [],
    "cell_id": "00001-68167f2e-360c-4e5c-88e3-c8f538141af2",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-8019e093-e6a5-41d5-aae4-2de4acf4e289",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3fb64d3e",
    "execution_start": 1637244541175,
    "execution_millis": 2056,
    "allow_embed": "code_output",
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer  #Librerias necesarias\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport pandas as pd\nimport pickle  #Pickle python es una herramienta para convertir objetos en una secuencia de bytes\n\n\ndef tokenize(sentence): # Declaramos un array tokens \n  tokens=[]  \n\n  for i in range(len(sentence)): \n    tokens.append(sentence[i])  # Separa los parrafos en palabras o 'tokens'\n  return tokens\n\n\n\n\ndef main():\n    Data = pd.read_csv(r\"datasetHP.csv\", error_bad_lines=False) #Leemos el dataset con pandas\n    \n    train_text, test_text, train_labels, test_labels = train_test_split(Data[\"Campo\"], #establecemos las variables\n                                                                    Data[\"Etiqueta\"],  # de test y train\n                                                                    random_state=85)\n    \n    \n    #El tfidVectorizer es un modelo que compara la frecuencia en que se repite una palabra\n    Tv = TfidfVectorizer( min_df= 20, max_features=5731, strip_accents='unicode',analyzer='char',ngram_range=(1,18), tokenizer=tokenize)\n    train_X = Tv.fit_transform(train_text).toarray() #armamos el array de entrenamiento\n    X_test = Tv.transform(test_text).toarray() #array de datos test\n  \n    \n    classNb = MultinomialNB() #inicializamos el modelo predictivo Bayers multinominal\n    classNb.fit(train_X, train_labels) #alimentamos el modelo con las variables de entrenaiento\n    \n    \n    predictionNB = classNb.predict(X_test) # predecimos usando las variables de prueba test\n \n    \n    accuracyNB = accuracy_score(test_labels, predictionNB) # Sacamos el porcentaje de predicción\n    print(f\"Accuracy NB: {accuracyNB:.4%}\") #mostramos en pantalla\n    \n    filenameTV = 'finalized_TV.sav' \n    pickle.dump(Tv, open(filenameTV, 'wb')) #de este modo guardamos nuestro modelo entrenado en bytes\n\n    filename = 'finalized_ClassNB.sav'\n    pickle.dump(classNb, open(filename, 'wb')) #de este modo guardamos nuestro modelo entrenado en bytes\n\nmain()\n    ",
   "outputs": [
    {
     "name": "stdout",
     "text": "Accuracy NB: 97.6971%\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-b6ede362-58c1-44c6-8256-6278c77ede6b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a2825195",
    "execution_start": 1637244543231,
    "execution_millis": 15,
    "allow_embed": "code",
    "deepnote_cell_type": "code"
   },
   "source": "# la variable corpus va a tener todo el texto o cuerpo del ticket\n\ncorpus = '''Ubicación : ***** - BXXX01 - *****\nHola buen dia equipo de IT , nos pueden ayudar a eliminar las \nsiguientes piezas de las direcciones,\ndebido a que el operador ingreso las piezas equivocadas.\nSKU          cantidad   dirección\nMNHW58683     90        Z-0-002-000   \nEINL12081      9            \nZAZV12355      4    \n                        M-0-005-011\nUYEX11111      2         \nttqe99514      55   \nXete00000      11  \n\nMuchas gracias por su ayuda\nSaludos\n'''    ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-92da9212-f063-4af8-9830-9d3ef77f8fde",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "415153b0",
    "execution_start": 1637244543251,
    "execution_millis": 4663,
    "is_code_hidden": false,
    "allow_embed": "output",
    "is_output_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "\n#  definimos una funcion que utilizará nuestro modelo entrenado\n# para clasificar el texto de la variable corpus\ndef bot(sentences,classNb,Tv): \n\n    text1 = sentences \n\n    examples = [\n        text1     #guardamos la palara o token en un array\n    ]\n\n    examples_X = Tv.transform(examples) #aplicamos el TfidfVectorizer\n    \n   \n    predict = classNb.predict(examples_X) #ejecutamos el modelo en nuestro token\n \n    result = []\n    for text, label in zip(examples, predict):\n       result ={\n           \n           label:text  #devolvera un Dir con la etiqueta y el token\n                \n        }\n    return result  \n\n\n#Declaramos la funcion cleaner, esta nos ayudara a quitar caracteres que no necesitamos\ndef cleaner(text): \n    #los signos que pueden hacer que nuestro modelo falle\n    signal = ['.','#',')','(',',',\"'\",'\"',';','[',']','}','{','/','?','=','+','@','*','+','<','>',':','¡','_']\n    #iteramos en el array de signal\n    for i in signal:\n       t = [char for char in text if char != str(i)] #En resumen el siguiente comprehension list agrega cada \n       text = t                                      #Caracter de la palabra solo si dicho caracter es distinto \n                                                     # a algun signo del array \n    return text\n\ndef Display(List): # Funcion que nos despliega la cadena de texto debido a que el cleaner nos devuelve un array\n   display = ''  #Inicializamos variable\n   for i in range(len(List)): #iteramos en la lista o array\n      char = List[i]\n      display = display + char #Cadena de caracteres\n      \n   return display\n\n\n\ndef mainOut():\n    AnswerBool = False\n    classNb =  pickle.load(open(\"finalized_ClassNB.sav\", 'rb')) #Con pickle cargamos el obecto\n    Tv =  pickle.load(open(\"finalized_TV.sav\", 'rb'))\n\n    while(not AnswerBool): #Bucle mientras que el boolean sea False este se ejecutara continuamente\n        try:\n             #Desplegamos un pequeño menu, en este caso el bot ejecutaria un POST/GET/PUT en una api y\n             #como entrada a esa API vamos a tener los items que va a extraer del texto del ticket\n            print('''\n                     1 .-  Retirar Error Sistemico - [\"SKU\",\"ADDRESS\",QUANTITY,\"WAREHOUSE\"]\n                     2 .-  detalles de direcciones - WAREHOUSE,ADDRESS \n                     3 .-  Direcciones Trabadas - ADDRESS,SKU,QUANTITY \n                  ''')\n\n            option = int(input('elige uno:')) #un input que espera del usuario\n          \n    \n            items = {               #Declaramos la variable Dir donde se guardarán nuestro resutados\n                     'none':0,       # en este caso NONE serán todos los tokens o palabras que no son utiles\n                     'Sku':0,         \n                     'Address':0,\n                     'Warehouse':0,\n                     'Quantity': 0,\n                }\n             \n            for i in corpus.split():  #Empezamos a iterar en la variable corpus\n\n                t = cleaner(i)   #limpiamos nuestra variable de signos indeseados \n                txt = Display(t) #Cleaner nos devuelve un array por lo tanto la funcion display \n                                  #nos devuelve la cadena de texto\n                \n\n                items.update(bot(txt.upper(),classNb,Tv)) #ejecutando la funcion bot ingresaremos informacion\n                                                          #a la variable Dir\n                \n                #Luego dependiendo de la opcion que se haya elegido entrará\n                #y completará en este caso el input a la API \n                if (option == 1):\n                   \n                   if items['Address'] != 0 and items['Address'] != '-' and items['Warehouse'] != 0 and items['Quantity'] != 0 and items['Sku'] != 0:\n                     #Print luego de validar que todos los campos esten correctos \n                      print('[\"'+str(items['Sku'])+'\",\"'+str(items['Address'])+'\",'+str(items['Quantity'])+',\"'+str(items['Warehouse'])+'\"],')\n                      items['Quantity'] = 0  \n                     \n                elif (option == 2 ):\n                     if items['Address'] != 0 and items['Address'] != '-' and items['Warehouse'] != 0:\n                       #Print luego de validar que todos los campos esten correctos   \n                       print(str(items['Warehouse'])+','+str(items['Address']))\n                       items['Quantity'] = 0 \n\n\n                elif (option == 3):\n                     \n                     if items['Address'] != 0 and items['Address'] != '-' and items['Sku'] != 0 and items['Quantity'] != 0:\n                         #Print luego de validar que todos los campos esten correctos \n                         print(str(items['Address'])+','+str(items['Sku'])+','+str(items['Quantity'])) \n                         items['Quantity'] = 0\n\n                AnswerBool = True #Sale del ciclo while\n                \n        except ValueError as e: \n            print(f'Error :{e}' ) #en caso de enviar campos  equivocados\n\n\nmainOut()",
   "outputs": [
    {
     "name": "stdout",
     "text": "\n                     1 .-  Retirar Error Sistemico - [\"SKU\",\"ADDRESS\",QUANTITY,\"WAREHOUSE\"]\n                     2 .-  detalles de direcciones - WAREHOUSE,ADDRESS \n                     3 .-  Direcciones Trabadas - ADDRESS,SKU,QUANTITY \n                  \n[\"MNHW58683\",\"Z-0-002-000\",90,\"BXXX01\"],\n[\"EINL12081\",\"Z-0-002-000\",9,\"BXXX01\"],\n[\"ZAZV12355\",\"Z-0-002-000\",4,\"BXXX01\"],\n[\"UYEX11111\",\"M-0-005-011\",2,\"BXXX01\"],\n[\"TTQE99514\",\"M-0-005-011\",55,\"BXXX01\"],\n[\"XETE00000\",\"M-0-005-011\",11,\"BXXX01\"],\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=8a83cba6-d8be-468d-939a-6d6572909c44' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "f320edcd-196c-4bd8-89f5-97211e5c4559",
  "deepnote_execution_queue": []
 }
}